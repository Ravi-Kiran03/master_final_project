MSc Project Artefact Code
Student: Ravi Kiran Malishetti (23037810)
================================================================================


--------------------------------------------------------------------------------
SCRIPT: 01_data_exploration.py
--------------------------------------------------------------------------------

# Data Exploration Script for RQ1 that prints dataset overview and saves simple visualizations
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# This script directory
SCRIPT_RESULTS_NAME = "01_data_exploration"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Setting Project root = one level up from scripts
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Data file path
DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'kc1_raw.csv')

# Results path
# Results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)

# Create folder if it does not exist
os.makedirs(RESULTS_PATH, exist_ok=True)


# Load dataset and print basic info
df = pd.read_csv(DATA_PATH)

print("First 5 rows of the dataset:")
print(df.head(), "\n")

print("Dataset shape (rows, columns):", df.shape, "\n")

print("Missing values per column:")
print(df.isnull().sum(), "\n")

print("Class distribution (counts):")
print(df['defects'].value_counts())

# Percentages
print("\nClass distribution (percentages):")
print(df['defects'].value_counts(normalize=True) * 100)

# Create plot figure for class distribution
plt.figure(figsize=(6,4))
ax = sns.countplot(x='defects', data=df)

# Title
plt.title("Defective vs Non-Defective Modules")

# Rename x-axis labels if desired
ax.set_xticklabels(['Non-Defective', 'Defective'])

# Add counts + percentages on top of bars
total = len(df)  # total number of rows
for p in ax.patches:
    height = p.get_height()
    percentage = height / total * 100
    ax.annotate(f'{height} ({percentage:.1f}%)',  # show count + percentage
                (p.get_x() + p.get_width() / 2, height), 
                ha='center', 
                va='bottom') 

plt.tight_layout()  # adjust layout to prevent clipping

# Save plot
plot_path = os.path.join(RESULTS_PATH, "01_class_distribution.png")
plt.savefig(plot_path)
plt.close()

print(f"Class distribution plot saved to {plot_path}")


--------------------------------------------------------------------------------
SCRIPT: 02_data_split.py
--------------------------------------------------------------------------------

# 02_data_split.py - Split dataset + scale features
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

SCRIPT_RESULTS_NAME = "02_data_split"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'kc1_raw.csv')
# Results root folder (common for all scripts)
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')  # or 'results'

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

df = pd.read_csv(DATA_PATH)

# Basic check
if df.isnull().sum().sum() > 0:
    print("Warning: Dataset contains missing values!")

# Features/target
X = df.drop(columns=['defects'])
y = df['defects']

print("Features shape:", X.shape)
print("Target shape:", y.shape)

# Class distribution check
print("\nClass distribution in full dataset:")
print(y.value_counts())

# Train-test split (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("\nTraining set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

print("\nClass distribution in training set:")
print(y_train.value_counts())

print("\nClass distribution in testing set:")
print(y_test.value_counts())

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save scaled datasets
pd.DataFrame(X_train_scaled, columns=X.columns).to_csv(
    os.path.join(RESULTS_PATH, 'X_train.csv'), index=False
)
pd.DataFrame(X_test_scaled, columns=X.columns).to_csv(
    os.path.join(RESULTS_PATH, 'X_test.csv'), index=False
)
y_train.to_csv(os.path.join(RESULTS_PATH, 'y_train.csv'), index=False)
y_test.to_csv(os.path.join(RESULTS_PATH, 'y_test.csv'), index=False)

# Save the scaler
joblib.dump(scaler, os.path.join(RESULTS_PATH, 'scaler.pkl'))

print(f"\nSaved to {RESULTS_PATH}")


# Plot class distribution in training set with counts + percentages
plt.figure(figsize=(6,4))
ax = sns.countplot(x=y_train)

# Title
plt.title("Training Set Class Distribution")

# Rename x-axis labels
ax.set_xticklabels(['Non-Defective', 'Defective'])

# Add counts + percentages on top of bars
total = len(y_train)
for p in ax.patches:
    height = p.get_height()
    percentage = height / total * 100
    ax.annotate(f'{height} ({percentage:.1f}%)',
                (p.get_x() + p.get_width() / 2, height),
                ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(RESULTS_PATH, "02_class_distribution_train_set.png"))
plt.close()


# Plot class distribution in testing set with counts + percentages
plt.figure(figsize=(6,4))
ax = sns.countplot(x=y_test)

# Title
plt.title("Testing Set Class Distribution")

# Rename x-axis labels
ax.set_xticklabels(['Non-Defective', 'Defective'])

# Add counts + percentages on top of bars
total = len(y_test)
for p in ax.patches:
    height = p.get_height()
    percentage = height / total * 100
    ax.annotate(f'{height} ({percentage:.1f}%)',
                (p.get_x() + p.get_width() / 2, height),
                ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(RESULTS_PATH, "02_class_distribution_test_set.png"))
plt.close()



--------------------------------------------------------------------------------
SCRIPT: 03_apply_SMOTE.py
--------------------------------------------------------------------------------

# 03_apply_smote.py
import os
import pandas as pd
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

SCRIPT_RESULTS_NAME = "03_apply_smote"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))  # folder of this script
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

# Load the preprocessed split datasets from previous script folder
X_train = pd.read_csv(os.path.join(RESULTS_ROOT, '02_data_split', 'X_train.csv'))
y_train = pd.read_csv(os.path.join(RESULTS_ROOT, '02_data_split', 'y_train.csv'))
y_train = y_train.values.ravel()


print("Original training set class distribution:")
print(pd.Series(y_train).value_counts())

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print("\nAfter SMOTE, class distribution:")
print(pd.Series(y_train_balanced).value_counts())

# Save the balanced dataset
pd.DataFrame(X_train_balanced, columns=X_train.columns).to_csv(
    os.path.join(RESULTS_PATH, 'X_train_balanced.csv'), index=False
)
pd.DataFrame(y_train_balanced, columns=['defects']).to_csv(
    os.path.join(RESULTS_PATH, 'y_train_balanced.csv'), index=False
)

print(f"\nBalanced training set saved to {RESULTS_PATH}")

plt.figure(figsize=(6,4))
ax = sns.countplot(x=y_train_balanced)
plt.title("Class Distribution after SMOTE")
ax.set_xticklabels(['Non-Defective', 'Defective'])

total = len(y_train_balanced)
for p in ax.patches:
    height = p.get_height()
    percentage = height / total * 100
    ax.annotate(f'{height} ({percentage:.1f}%)',
                (p.get_x() + p.get_width() / 2, height),
                ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(RESULTS_PATH, "03_class_distribution_after_smote.png"))
plt.close()



--------------------------------------------------------------------------------
SCRIPT: 04_train_random_forest_model.py
--------------------------------------------------------------------------------


import os
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier

# Script-specific results folder name
SCRIPT_RESULTS_NAME = "04_train_random_forest"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

# Load balanced training data from previous script
X_train = pd.read_csv(os.path.join(RESULTS_ROOT, '03_apply_smote', 'X_train_balanced.csv'))
y_train = pd.read_csv(os.path.join(RESULTS_ROOT, '03_apply_smote', 'y_train_balanced.csv')).values.ravel()


# Initialize and train Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100, # number of trees
    max_depth=None, # no maximum depth
    random_state=42, # for reproducibility
    n_jobs=-1 # utilize all available cores
)
rf_model.fit(X_train, y_train)

# Save trained model
model_path = os.path.join(RESULTS_PATH, 'random_forest_rq1.pkl')
joblib.dump(rf_model, model_path)

print(f"Random Forest model trained and saved to {model_path}")



--------------------------------------------------------------------------------
SCRIPT: 05_evaluate_model.py
--------------------------------------------------------------------------------

import os
import pandas as pd
import numpy as np
import joblib
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    confusion_matrix, ConfusionMatrixDisplay,
    roc_curve, roc_auc_score, classification_report
)
import matplotlib.pyplot as plt


# Script-specific results folder
SCRIPT_RESULTS_NAME = "05_evaluate_random_forest"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

# Load model and test data
MODEL_PATH = os.path.join(RESULTS_ROOT, '04_train_random_forest', 'random_forest_rq1.pkl')
X_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'X_test.csv')
Y_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'y_test.csv')

X_test = pd.read_csv(X_TEST_PATH)
y_test = pd.read_csv(Y_TEST_PATH).values.ravel()
model = joblib.load(MODEL_PATH)
print("Model loaded successfully!\n")



# Make predictions
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]  # For AUC & ROC


# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)

print("\n===== Model Evaluation Results on TEST DATA =====\n")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-score:  {f1:.4f}")
print(f"AUC:       {auc:.4f}\n")


# Save metrics to file
metrics_output_path = os.path.join(RESULTS_PATH, "evaluation_metrics.txt")
with open(metrics_output_path, "w") as f:
    f.write("Model Performance on Test Data\n")
    f.write("-----------------------------------\n")
    f.write(f"Accuracy:  {accuracy:.4f}\n")
    f.write(f"Precision: {precision:.4f}\n")
    f.write(f"Recall:    {recall:.4f}\n")
    f.write(f"F1-score:  {f1:.4f}\n")
    f.write(f"AUC:       {auc:.4f}\n")

print(f"Performance metrics saved to {metrics_output_path}")


# Classification Report
report = classification_report(y_test, y_pred, target_names=["Non-Defective", "Defective"])

report_path = os.path.join(RESULTS_PATH, "classification_report.txt")
with open(report_path, "w") as f:
    f.write(report)

print(f"Classification report saved to {report_path}\n")


# Confusion Matrix Plot
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])

# Count total samples per class
unique, counts = np.unique(y_test, return_counts=True)
class_totals = dict(zip(unique, counts))  # {0: 357, 1: 65}

# Axis labels with totals
labels_with_totals = [
    f"Non-Defective ({class_totals[0]})",
    f"Defective ({class_totals[1]})"
]

# Cell text labels
cell_text = [
    [f"{cm[0,0]} (True Non-Defective)", f"{cm[0,1]} (False Non-Defective)"],
    [f"{cm[1,0]} (False Defective)", f"{cm[1,1]} (True Defective)"]
]

# Plot
fig, ax = plt.subplots(figsize=(10,7))
im = ax.imshow(cm, cmap='Blues')

# Title
ax.set_title("Random Forest Confusion Matrix - Test Set", fontsize=14)

# Tick labels
ax.set_xticks(np.arange(len(labels_with_totals)))
ax.set_yticks(np.arange(len(labels_with_totals)))
ax.set_xticklabels(labels_with_totals, rotation=45, ha="right")
ax.set_yticklabels(labels_with_totals)

# Add text in each cell
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j, i, cell_text[i][j],
                ha="center", va="center", color="black", fontsize=10)

# Axis labels
ax.set_ylabel("Actual")
ax.set_xlabel("Predicted")
plt.tight_layout()

# Save
plt.savefig(os.path.join(RESULTS_PATH, "confusion_matrix_test_set.png"))
plt.show()
plt.close()
print("Confusion matrix with detailed cell labels saved.")



# ROC Curve Plot
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

plt.figure()
plt.plot(fpr, tpr, linewidth=2)
plt.plot([0, 1], [0, 1], linestyle='--')  # diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.savefig(os.path.join(RESULTS_PATH, "roc_curve_test.png"))
plt.close()

print("ROC curve saved successfully!")

print("\nEvaluation complete!")



--------------------------------------------------------------------------------
SCRIPT: 06_create_missing_data.py
--------------------------------------------------------------------------------

# 06_create_missing_data.py
import os
import pandas as pd
import numpy as np


# Script-specific results folder
SCRIPT_RESULTS_NAME = "06_create_missing_data"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

# Load test data from previous script
DATA_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'X_test.csv')
X_test = pd.read_csv(DATA_PATH)

# Function to introduce MCAR missingness

def introduce_mcar(df, missing_pct, random_state=42):
    """
    Introduces Missing Completely At Random (MCAR) in the dataset.
    
    :param df: input DataFrame
    :param missing_pct: fraction of values to set as NaN (0-100)
    :param random_state: random seed
    :return: DataFrame with missing values
    """
    np.random.seed(random_state)
    df_missing = df.copy()
    total_values = df_missing.size
    n_missing = int(total_values * (missing_pct / 100))
    
    # Randomly choose indices for missing values
    missing_indices = (
        np.random.choice(total_values, n_missing, replace=False)
    )
    
    # Convert 1D indices to 2D indices for DataFrame
    rows, cols = np.unravel_index(missing_indices, df_missing.shape)
    df_missing.values[rows, cols] = np.nan
    
    return df_missing


# Generate missing datasets at 10%, 20%, 30%, 40%

missing_levels = [10, 20, 30, 40]

for pct in missing_levels:
    df_missing = introduce_mcar(X_test, pct)
    # Save in the script-specific folder under scripts_results
    save_path = os.path.join(RESULTS_PATH, f'X_test_missing_{pct}pct.csv')
    df_missing.to_csv(save_path, index=False)
    print(f"Saved {pct}% missing data to {save_path}")



--------------------------------------------------------------------------------
SCRIPT: 07_impute_missing_data.py
--------------------------------------------------------------------------------

# 07_impute_missing_data.py
import os
import pandas as pd
from sklearn.impute import SimpleImputer

# Script-specific results folder
SCRIPT_RESULTS_NAME = "07_impute_missing_data"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

# Missing datasets are loaded from previous script folder
MISSING_FOLDER = os.path.join(RESULTS_ROOT, '06_create_missing_data')

# Output folder for imputed datasets (inside this script's folder)
OUTPUT_FOLDER = RESULTS_PATH
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

missing_levels = [10, 20, 30, 40]

imputer = SimpleImputer(strategy='mean')

for pct in missing_levels:
    file_path = os.path.join(MISSING_FOLDER, f"X_test_missing_{pct}pct.csv")
    df_missing = pd.read_csv(file_path)

    # Apply mean imputation
    df_imputed = pd.DataFrame(imputer.fit_transform(df_missing),
                              columns=df_missing.columns)

    # Save imputed file
    save_path = os.path.join(OUTPUT_FOLDER, f"X_test_missing_{pct}pct_imputed.csv")
    df_imputed.to_csv(save_path, index=False)

    print(f"Saved imputed dataset: {save_path}")



--------------------------------------------------------------------------------
SCRIPT: 08_evaluate_missing_data.py
--------------------------------------------------------------------------------

# 08_evaluate_missing_data.py

import os
import pandas as pd
import joblib
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score


# Script-specific results folder
SCRIPT_RESULTS_NAME = "08_evaluate_missing_data"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)


# Paths to previous results
MODEL_PATH = os.path.join(RESULTS_ROOT, '04_train_random_forest', 'random_forest_rq1.pkl')
X_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'X_test.csv')
Y_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'y_test.csv')

IMPUTED_FOLDER = os.path.join(RESULTS_ROOT, '07_impute_missing_data')
OUTPUT_CSV = os.path.join(RESULTS_PATH, 'missingness_results.csv')
PLOT_PATH = os.path.join(RESULTS_PATH, 'missingness_performance.png')


# Load model and test labels
model = joblib.load(MODEL_PATH)
X_test_original = pd.read_csv(X_TEST_PATH)
y_test = pd.read_csv(Y_TEST_PATH).values.ravel()

# Evaluate different missingness levels
missing_levels = [10, 20, 30, 40]
results = []

for pct in missing_levels:
    imputed_file = os.path.join(IMPUTED_FOLDER, f"X_test_missing_{pct}pct_imputed.csv")
    X_test_imputed = pd.read_csv(imputed_file)

    # Predict
    y_pred = model.predict(X_test_imputed)

    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    results.append({
        "missingness_pct": pct,
        "accuracy": accuracy,
        "f1_score": f1,
        "precision": precision,
        "recall": recall
    })

    print(f"Missingness {pct}% → Accuracy: {accuracy:.4f}, F1: {f1:.4f}, "
          f"Precision: {precision:.4f}, Recall: {recall:.4f}")

# Save results to CSV
results_df = pd.DataFrame(results)
results_df.to_csv(OUTPUT_CSV, index=False)
print("\nSaved results to:")
print(OUTPUT_CSV)

# Plot performance vs missingness
plt.figure(figsize=(8,5))
plt.plot(results_df['missingness_pct'], results_df['accuracy'], marker='o', label='Accuracy')
plt.plot(results_df['missingness_pct'], results_df['f1_score'], marker='o', label='F1 Score')
plt.plot(results_df['missingness_pct'], results_df['precision'], marker='o', label='Precision')
plt.plot(results_df['missingness_pct'], results_df['recall'], marker='o', label='Recall')
plt.xlabel('Missingness (%)')
plt.ylabel('Score')
plt.title('Random Forest Performance vs Missingness')
plt.xticks(missing_levels)
plt.ylim(0, 1)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig(PLOT_PATH)
plt.close()

print(f"Performance plot saved to: {PLOT_PATH}")



--------------------------------------------------------------------------------
SCRIPT: 09_train_svm.py
--------------------------------------------------------------------------------

import os
import pandas as pd
import joblib
from sklearn.svm import SVC


# Paths Setup
SCRIPT_RESULTS_NAME = "09_train_svm"  # give a unique folder name

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

X_TRAIN_PATH = os.path.join(RESULTS_ROOT, '03_apply_smote', 'X_train_balanced.csv')
Y_TRAIN_PATH = os.path.join(RESULTS_ROOT, '03_apply_smote', 'y_train_balanced.csv')

MODEL_SAVE_PATH = os.path.join(RESULTS_PATH, 'svm_rq1.pkl')


# Load Train Data
print("Loading training data...")
X_train = pd.read_csv(X_TRAIN_PATH)
y_train = pd.read_csv(Y_TRAIN_PATH).values.ravel()

print("Training data loaded successfully!")


# Train SVM Model
print("\nTraining SVM model...")
svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)

print("Model training completed!")


# Save Model
joblib.dump(svm_model, MODEL_SAVE_PATH)
print(f"\nSVM model saved successfully at:\n{MODEL_SAVE_PATH}")



--------------------------------------------------------------------------------
SCRIPT: 10_evaluate_svm.py
--------------------------------------------------------------------------------

import os
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import numpy as np

from sklearn.metrics import (
    accuracy_score, f1_score, classification_report,
    confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, roc_auc_score
)


# Paths Setup
SCRIPT_RESULTS_NAME = "10_evaluate_svm"  # unique folder name

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific results folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

MODEL_PATH = os.path.join(RESULTS_ROOT, '09_train_svm', 'svm_rq1.pkl')

# Test data comes from previous data split script
X_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'X_test.csv')
Y_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'y_test.csv')

# Paths for outputs inside this script-specific folder
REPORT_SAVE_PATH = os.path.join(RESULTS_PATH, 'svm_test_results.txt')
CONF_MATRIX_PLOT = os.path.join(RESULTS_PATH, 'svm_test_confusion_matrix.png')
# Load Model and Test Data

print("Loading test data and SVM model...")
svm_model = joblib.load(MODEL_PATH)

X_test = pd.read_csv(X_TEST_PATH)
y_test = pd.read_csv(Y_TEST_PATH).values.ravel()
print("Data loaded successfully!")



# Predict
print("\nPredicting test data...")
y_pred = svm_model.predict(X_test)
y_prob = svm_model.predict_proba(X_test)[:, 1]  # needed for AUC



# Evaluate Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_prob)

class_report = classification_report(y_test, y_pred, target_names=["Non-Defective", "Defective"])


print("\n===== SVM Performance on TEST DATA =====")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-score:  {f1:.4f}")
print(f"AUC:       {auc:.4f}")
print("\nClassification Report:")
print(class_report)



# Save Results to txt file
with open(REPORT_SAVE_PATH, "w") as file:
    file.write("SVM Model Evaluation on Test Data\n")
    file.write("-----------------------------------\n")
    file.write(f"Accuracy:  {accuracy:.4f}\n")
    file.write(f"Precision: {precision:.4f}\n")
    file.write(f"Recall:    {recall:.4f}\n")
    file.write(f"F1-score:  {f1:.4f}\n")
    file.write(f"AUC:       {auc:.4f}\n\n")
    file.write("Classification Report\n")
    file.write(class_report)

print(f"\nSaved evaluation results to {REPORT_SAVE_PATH}")



# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])

# Total samples per class
total_per_class = np.bincount(y_test)  # [total Non-Defective, total Defective]

# Axis labels with totals
classes = ["Non-Defective", "Defective"]
labels_with_totals = [f"{classes[i]} ({total_per_class[i]})" for i in range(len(classes))]

# Cell labels with True/False annotations
cell_text = [
    [f"{cm[0,0]} (True Non-Defective)", f"{cm[0,1]} (False Non-Defective)"],
    [f"{cm[1,0]} (False Defective)", f"{cm[1,1]} (True Defective)"]
]

# Create figure
fig, ax = plt.subplots(figsize=(8,6))
im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
#plt.colorbar(im, ax=ax)

# Set ticks and labels
ax.set_xticks(np.arange(len(classes)))
ax.set_yticks(np.arange(len(classes)))
ax.set_xticklabels(labels_with_totals, rotation=45, ha="right", rotation_mode="anchor")
ax.set_yticklabels(labels_with_totals)

# Add cell annotations
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j, i, cell_text[i][j], ha="center", va="center", color="black", fontsize=10)

# Axis labels and title
ax.set_xlabel("Predicted")
ax.set_ylabel("Actual")
ax.set_title("SVM Confusion Matrix - Test Data")

plt.tight_layout()
plt.savefig(CONF_MATRIX_PLOT)
plt.show()
plt.close()

print(f"Confusion matrix with totals and descriptive cell labels saved to {CONF_MATRIX_PLOT}")


--------------------------------------------------------------------------------
SCRIPT: 11_evaluate_missing_data_svm.py
--------------------------------------------------------------------------------

# 08_evaluate_missing_data_svm.py

import os
import pandas as pd
import joblib
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score


# Paths
SCRIPT_RESULTS_NAME = "11_evaluate_missing_data_svm"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root folder
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# Script-specific folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)


# Paths for model and data
MODEL_PATH = os.path.join(RESULTS_ROOT, '09_train_svm', 'svm_rq1.pkl')  # change if needed
X_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'X_test.csv')
Y_TEST_PATH = os.path.join(RESULTS_ROOT, '02_data_split', 'y_test.csv')

IMPUTED_FOLDER = os.path.join(os.path.dirname(RESULTS_PATH), '07_impute_missing_data')
OUTPUT_CSV = os.path.join(RESULTS_PATH, 'missingness_results_svm.csv')
PLOT_PATH = os.path.join(RESULTS_PATH, 'missingness_performance_svm.png')


# Load model and test labels
model = joblib.load(MODEL_PATH)
X_test_original = pd.read_csv(X_TEST_PATH)
y_test = pd.read_csv(Y_TEST_PATH).values.ravel()


# Evaluate different missingness levels
missing_levels = [10, 20, 30, 40]
results = []

for pct in missing_levels:
    imputed_file = os.path.join(IMPUTED_FOLDER, f"X_test_missing_{pct}pct_imputed.csv")
    X_test_imputed = pd.read_csv(imputed_file)

    # Predict using SVM model
    y_pred = model.predict(X_test_imputed)

    # Compute metrics
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    results.append({
        "missingness_pct": pct,
        "accuracy": accuracy,
        "f1_score": f1,
        "precision": precision,
        "recall": recall
    })

    print(f"Missingness {pct}% → Accuracy: {accuracy:.4f}, F1: {f1:.4f}, "
          f"Precision: {precision:.4f}, Recall: {recall:.4f}")


# Save results into CSV
results_df = pd.DataFrame(results)
results_df.to_csv(OUTPUT_CSV, index=False)

print("\nSaved results to:")
print(OUTPUT_CSV)

# Plot performance vs missingness
plt.figure(figsize=(8,5))
plt.plot(results_df['missingness_pct'], results_df['accuracy'], marker='o', label='Accuracy')
plt.plot(results_df['missingness_pct'], results_df['f1_score'], marker='o', label='F1 Score')
plt.plot(results_df['missingness_pct'], results_df['precision'], marker='o', label='Precision')
plt.plot(results_df['missingness_pct'], results_df['recall'], marker='o', label='Recall')
plt.xlabel('Missingness (%)')
plt.ylabel('Score')
plt.title('SVM Performance vs Missingness')
plt.xticks(missing_levels)
plt.ylim(0, 1)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig(PLOT_PATH)
plt.close()

print(f"Performance plot saved to: {PLOT_PATH}")



--------------------------------------------------------------------------------
SCRIPT: 12_compare_missingness_models.py
--------------------------------------------------------------------------------

# 12_compare_missingness_models.py

import os
import pandas as pd
import matplotlib.pyplot as plt


# Script-specific results folder
SCRIPT_RESULTS_NAME = "12_compare_missingness_models"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)


# Input CSVs from previous scripts
RF_RESULTS = os.path.join(RESULTS_ROOT, '08_evaluate_missing_data', 'missingness_results.csv')
SVM_RESULTS = os.path.join(RESULTS_ROOT, '11_evaluate_missing_data_svm', 'missingness_results_svm.csv')

OUTPUT_PLOT = os.path.join(RESULTS_PATH, 'comparison_missingness_performance_full.png')
OUTPUT_CSV = os.path.join(RESULTS_PATH, 'comparison_missingness_table.csv')


# Load result files

df_rf = pd.read_csv(RF_RESULTS)
df_svm = pd.read_csv(SVM_RESULTS)

missing_levels = df_rf['missingness_pct']

#CSV

comparison_data = []

for i, pct in enumerate(missing_levels):
    row = {
        "Missingness (%)": pct,
        "RF Accuracy": df_rf.loc[i, 'accuracy'],
        "SVM Accuracy": df_svm.loc[i, 'accuracy'],
        "RF F1": df_rf.loc[i, 'f1_score'],
        "SVM F1": df_svm.loc[i, 'f1_score'],
        "RF Precision": df_rf.loc[i, 'precision'],
        "SVM Precision": df_svm.loc[i, 'precision'],
        "RF Recall": df_rf.loc[i, 'recall'],
        "SVM Recall": df_svm.loc[i, 'recall'],
    }

    desc = []
    if df_rf.loc[i, 'accuracy'] > df_svm.loc[i, 'accuracy']:
        desc.append("RF more robust")
    elif df_rf.loc[i, 'accuracy'] < df_svm.loc[i, 'accuracy']:
        desc.append("SVM more robust")
    else:
        desc.append("Similar accuracy")

    if df_rf.loc[i, 'f1_score'] > df_svm.loc[i, 'f1_score']:
        desc.append("RF better F1")
    elif df_rf.loc[i, 'f1_score'] < df_svm.loc[i, 'f1_score']:
        desc.append("SVM better F1")

    if pct >= 30:
        desc.append("Performance drops at high missingness")

    row["Description"] = "; ".join(desc)
    comparison_data.append(row)

df_comparison = pd.DataFrame(comparison_data)
df_comparison.to_csv(OUTPUT_CSV, index=False)

print("\nComparison table saved:")
print(OUTPUT_CSV)


# Plot
plt.figure(figsize=(10, 7))

# ---- Accuracy ----
plt.plot(missing_levels, df_rf['accuracy'], marker='o', linestyle='-', label='RF Accuracy')
plt.plot(missing_levels, df_svm['accuracy'], marker='o', linestyle='--', label='SVM Accuracy')

# ---- F1 ----
plt.plot(missing_levels, df_rf['f1_score'], marker='s', linestyle='-', label='RF F1')
plt.plot(missing_levels, df_svm['f1_score'], marker='s', linestyle='--', label='SVM F1')

# ---- Precision ----
plt.plot(missing_levels, df_rf['precision'], marker='^', linestyle='-', label='RF Precision')
plt.plot(missing_levels, df_svm['precision'], marker='^', linestyle='--', label='SVM Precision')

# ---- Recall ----
plt.plot(missing_levels, df_rf['recall'], marker='d', linestyle='-', label='RF Recall')
plt.plot(missing_levels, df_svm['recall'], marker='d', linestyle='--', label='SVM Recall')

plt.xlabel("Missingness (%)")
plt.ylabel("Score")
plt.title("Full Model Comparison vs Missingness (RF vs SVM)")
plt.xticks(missing_levels)
plt.ylim(0, 1)
plt.grid(True)
plt.legend()
plt.tight_layout()

plt.savefig(OUTPUT_PLOT)
plt.close()

print('\nComparison plot saved:')
print(OUTPUT_PLOT)



--------------------------------------------------------------------------------
SCRIPT: 13_shap_explanations.py
--------------------------------------------------------------------------------

# 13_shap_explanations_global_only.py
# Global SHAP explanations for Random Forest model (RQ2)
# Outputs:
#  - shap_summary_plot.png (global)
#  - shap_bar_plot.png (global)
#  - shap_global_importance.csv (ranked mean(|SHAP|))
#  - shap_global_summary.txt (top-N ranked list)

import os
import joblib
import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# -----------------------------
# Script-specific results folder
# -----------------------------
SCRIPT_RESULTS_NAME = "13_shap_explanations"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "..", ".."))

RESULTS_ROOT = os.path.join(PROJECT_ROOT, "scripts_results")
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

# -----------------------------
# Inputs from previous scripts
# -----------------------------
MODEL_PATH = os.path.join(RESULTS_ROOT, "04_train_random_forest", "random_forest_rq1.pkl")
X_TEST_PATH = os.path.join(RESULTS_ROOT, "02_data_split", "X_test.csv")
Y_TEST_PATH = os.path.join(RESULTS_ROOT, "02_data_split", "y_test.csv")  # optional, not used here

# -----------------------------
# Helpers
# -----------------------------
def get_class1_shap_values(explainer, X: pd.DataFrame) -> np.ndarray:
    """
    Returns SHAP values for the positive class (class index 1) in a robust way.
    Handles both:
      - New SHAP API: explainer(X) -> Explanation with .values shape:
            (n_samples, n_features)  [binary regression-style output]
         or (n_samples, n_features, n_outputs/classes)
      - Older behavior: explainer.shap_values(X) -> list/array
    """
    try:
        sv = explainer(X)  # New API preferred
        values = sv.values

        # If (n, m, 2) -> take class 1
        if values.ndim == 3:
            if values.shape[2] < 2:
                raise ValueError(f"Unexpected SHAP output shape: {values.shape}")
            return values[:, :, 1]

        # If (n, m) -> treat as already for the model output (often positive class / log-odds)
        if values.ndim == 2:
            return values

        raise ValueError(f"Unexpected SHAP values ndim: {values.ndim}, shape: {values.shape}")

    except Exception:
        # Fallback for older SHAP versions
        values = explainer.shap_values(X)

        # For classifiers, often a list of arrays [class0, class1]
        if isinstance(values, list) and len(values) >= 2:
            return np.asarray(values[1])

        # Or sometimes just an array (n, m) for binary output
        values = np.asarray(values)
        if values.ndim == 2:
            return values
        if values.ndim == 3 and values.shape[2] >= 2:
            return values[:, :, 1]

        raise ValueError(f"Could not interpret SHAP output; got type={type(values)} shape={getattr(values,'shape',None)}")


# -----------------------------
# Load model and test data
# -----------------------------
print("Loading model and test data...")
rf_model = joblib.load(MODEL_PATH)
X_test = pd.read_csv(X_TEST_PATH)
print("Model and data loaded successfully.")

# -----------------------------
# Initialize SHAP explainer
# -----------------------------
print("Initializing SHAP TreeExplainer...")
explainer = shap.TreeExplainer(rf_model)

print("Computing SHAP values (positive / defective class)...")
shap_values_class1 = get_class1_shap_values(explainer, X_test)
print(f"SHAP values computed. Shape: {shap_values_class1.shape}")

# -----------------------------
# Global importance numbers
# -----------------------------
mean_abs_shap = np.abs(shap_values_class1).mean(axis=0)  # (n_features,)
importance_df = pd.DataFrame(
    {"feature": X_test.columns, "mean_abs_shap": mean_abs_shap}
).sort_values("mean_abs_shap", ascending=False)

# Save CSV
importance_csv_path = os.path.join(RESULTS_PATH, "shap_global_importance.csv")
importance_df.to_csv(importance_csv_path, index=False)

# Save TXT summary (top N)
top_n = 10
summary_txt_path = os.path.join(RESULTS_PATH, "shap_global_summary.txt")
with open(summary_txt_path, "w") as f:
    f.write("SHAP Global Feature Importance (Random Forest)\n")
    f.write("Positive class: Defective (class 1)\n")
    f.write("=============================================\n\n")
    f.write(f"Top {top_n} features by mean(|SHAP|):\n\n")
    for rank, (_, row) in enumerate(importance_df.head(top_n).iterrows(), start=1):
        f.write(f"{rank:02d}. {row['feature']}: {row['mean_abs_shap']:.6f}\n")

print(f"Saved global SHAP importance CSV to: {importance_csv_path}")
print(f"Saved global SHAP importance TXT to: {summary_txt_path}")

# -----------------------------
# SHAP Summary Plot (Global)
# -----------------------------
print("Generating SHAP summary plot (global)...")
plt.figure()
shap.summary_plot(
    shap_values_class1,
    X_test,
    show=False
)
plt.title("SHAP Summary Plot (Defective Class)")
plt.tight_layout()
plt.savefig(os.path.join(RESULTS_PATH, "shap_summary_plot.png"), dpi=300)
plt.close()
print("Saved shap_summary_plot.png")

# -----------------------------
# SHAP Bar Plot (Global Importance)
# -----------------------------
print("Generating SHAP bar plot (global importance)...")
plt.figure()
shap.summary_plot(
    shap_values_class1,
    X_test,
    plot_type="bar",
    show=False
)
plt.title("Global Feature Importance Using SHAP (Defective Class)")
plt.tight_layout()
plt.savefig(os.path.join(RESULTS_PATH, "shap_bar_plot.png"), dpi=300)
plt.close()
print("Saved shap_bar_plot.png")

print("\nSHAP global explainability outputs generated successfully!")



--------------------------------------------------------------------------------
SCRIPT: 14_lime_explanations.py
--------------------------------------------------------------------------------

# 14_lime_explanations.py
# LIME explanations for Random Forest model (RQ2)

import os
import pandas as pd
import joblib
import matplotlib.pyplot as plt
from lime.lime_tabular import LimeTabularExplainer

# -----------------------------
# Script-specific results folder
# -----------------------------
SCRIPT_RESULTS_NAME = "14_lime_explanations"

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# Common results root
RESULTS_ROOT = os.path.join(PROJECT_ROOT, 'scripts_results')

# This script's output folder
RESULTS_PATH = os.path.join(RESULTS_ROOT, SCRIPT_RESULTS_NAME)
os.makedirs(RESULTS_PATH, exist_ok=True)

# -----------------------------
# Inputs from previous scripts
# -----------------------------
MODEL_PATH = os.path.join(
    RESULTS_ROOT,
    '04_train_random_forest',
    'random_forest_rq1.pkl'
)

X_TRAIN_PATH = os.path.join(
    RESULTS_ROOT,
    '02_data_split',
    'X_train.csv'
)

X_TEST_PATH = os.path.join(
    RESULTS_ROOT,
    '02_data_split',
    'X_test.csv'
)

Y_TEST_PATH = os.path.join(
    RESULTS_ROOT,
    '02_data_split',
    'y_test.csv'
)

# -----------------------------
# Load model and data
# -----------------------------
print("Loading model and data...")

rf_model = joblib.load(MODEL_PATH)
X_train = pd.read_csv(X_TRAIN_PATH)
X_test = pd.read_csv(X_TEST_PATH)
y_test = pd.read_csv(Y_TEST_PATH).values.ravel()

print("Model and data loaded successfully!\n")

# -----------------------------
# Initialize LIME explainer
# -----------------------------
print("Initializing LIME explainer...")

lime_explainer = LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns,
    class_names=["Non-Defective", "Defective"],
    mode="classification",
    discretize_continuous=True
)

print("LIME explainer initialized!\n")

# -----------------------------
# Explain one defective instance
# -----------------------------
print("Generating LIME explanation...")

# Pick first defective instance
positive_index = list(y_test).index(1)
instance = X_test.iloc[positive_index].values

exp = lime_explainer.explain_instance(
    data_row=instance,
    predict_fn=rf_model.predict_proba,
    num_features=10
)

# -----------------------------
# Save explanation as plot
# -----------------------------
# Extract feature names and contributions
feature_names = [f[0] for f in exp.as_list()]
contributions = [f[1] for f in exp.as_list()]

# Color bars by direction
colors = ['green' if val > 0 else 'red' for val in contributions]

plt.figure(figsize=(8,5))
plt.barh(feature_names, contributions, color=colors)
plt.xlabel("Feature Contribution to Prediction")
plt.title("Local Feature Contributions for Defective Module (LIME)")
plt.tight_layout()
plt.savefig(os.path.join(RESULTS_PATH, "lime_defective_instance.png"))
plt.close()

# -----------------------------
# Save explanation as text
# -----------------------------
with open(os.path.join(RESULTS_PATH, "lime_defective_instance.txt"), "w") as f:
    f.write(exp.as_list().__str__())

print("\nLIME explanation saved successfully to:")
print(RESULTS_PATH)


